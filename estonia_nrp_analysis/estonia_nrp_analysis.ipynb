{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and Statistical analysis of the Estonian National Reform Program Action Plan\n",
    "## TF-IDF, LDA Model, Summarization, Cosine Similarity, Word Embeddings\n",
    "\n",
    "I use the tools described above to supplement my human/manual analysis of the Estonia NRP-AP. The paper can be found at http://jeanmonnetchair.eubga.uom.gr/download/jean-monnet-essay-3-2020-podiotis/ . For word vectors used see https://nlp.stanford.edu/projects/glove/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import heapq\n",
    "from nltk import PorterStemmer, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.update([\"estonia\"])\n",
    "paragraphs_list = []\n",
    "processed_list =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = parser.from_file('nrp_estonia_2020_action_plan_2019-2020_30.05.2019.pdf')\n",
    "pdf_text = pdf['content']\n",
    "paragraphs_list = pdf_text.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in paragraphs_list:\n",
    "    paragraph_tokenized = []\n",
    "    paragraph_tokenized_lemmatized = []\n",
    "\n",
    "    # STRING PROCESSING\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    paragraph = paragraph.lower()\n",
    "    paragraph = re.sub(\"x \", \"\", paragraph)\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    paragraph = re.sub(\"ministry\", \"\", paragraph)\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    paragraph = re.sub(\"of education and research\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of culture\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of social affairs\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of economic affairs and communications\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of finance\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"government office\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of justice\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of rural affairs\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of the interior\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of the environment\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"of defence\", \"\", paragraph)\n",
    "    paragraph = re.sub(\"[\\d+()‘’“”\\%:\\*,&.\\-–;:!?__]\", \"\", paragraph)\n",
    "\n",
    "    # TOKENIZATON\n",
    "    paragraph_tokenized = word_tokenize(paragraph)\n",
    "\n",
    "    # POS TAGGING\n",
    "    tags = nltk.pos_tag(paragraph_tokenized)\n",
    "    paragraph_tagged = []\n",
    "    for i in tags:\n",
    "        if \"NN\" in i[1]:\n",
    "            paragraph_tagged.append(i[0])\n",
    "    paragraph_tokenized = paragraph_tagged\n",
    "\n",
    "\n",
    "    # LEMMATIZATION/STEMMING & STOPWORDS\n",
    "    for word in paragraph_tokenized:\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            #lemmatizer = WordNetLemmatizer()\n",
    "            #word = lemmatizer.lemmatize(word)\n",
    "            stemmer = PorterStemmer()\n",
    "            word = stemmer.stem(word)\n",
    "            paragraph_tokenized_lemmatized.append(word)\n",
    "\n",
    "\n",
    "\n",
    "    if len(paragraph_tokenized_lemmatized) > 0:\n",
    "        processed_list.append(paragraph_tokenized_lemmatized)\n",
    "\n",
    "del processed_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for paragraph in processed_list:\n",
    "    for word in paragraph:\n",
    "        if word not in wordfreq.keys():\n",
    "            wordfreq[word] = 1\n",
    "        else:\n",
    "            wordfreq[word] += 1\n",
    "most_freq = heapq.nlargest(100, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_values = {}\n",
    "for token in most_freq:\n",
    "    doc_containing_word = 0\n",
    "    for document in processed_list:\n",
    "        if token in document:\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(processed_list)/(1 + doc_containing_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values = {}\n",
    "for token in most_freq:\n",
    "    sent_tf_vector = []\n",
    "    for document in processed_list:\n",
    "        doc_freq = 0\n",
    "        for word in document:\n",
    "            if token == word:\n",
    "                  doc_freq += 1\n",
    "        word_tf = doc_freq/len(document)\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values_total = word_tf_values\n",
    "for key in word_tf_values:\n",
    "    average = 0\n",
    "    average = sum(word_tf_values[key]) / len(word_tf_values[key])\n",
    "    word_tf_values_total[key] = average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tfidf_values_total = word_tf_values\n",
    "for key in word_tf_values:\n",
    "    product = 0\n",
    "    product = word_tf_values[key] * word_idf_values[key]\n",
    "    word_tfidf_values_total[key] = product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          TF-IDF / Weight\n",
      "develop         0.0419833\n",
      "servic           0.028245\n",
      "activ           0.0489176\n",
      "measur          0.0373603\n",
      "project         0.0219841\n",
      "...                   ...\n",
      "unemploy       0.00495458\n",
      "insur          0.00608646\n",
      "strategi        0.0123694\n",
      "construct      0.00504765\n",
      "process        0.00543679\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "index = word_tfidf_values_total.keys()\n",
    "dftfid = pd.DataFrame(index=index, columns=[\"TF-IDF / Weight\"])\n",
    "for term in index:\n",
    "    dftfid.loc[term, \"TF-IDF / Weight\"] = word_tfidf_values_total[term]\n",
    "dftfid.to_excel(\"TF-IDF.xlsx\")\n",
    "print(dftfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndictionary = gensim.corpora.Dictionary(processed_list)\\ndf = pd.DataFrame(list(dictionary.items()), columns=[\"0\", \"word\"])\\ndf = df.drop([\"0\"], axis=1)\\n# BAG OF WORDS\\ncorpus = [dictionary.doc2bow(paragraph) for paragraph in processed_list]\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_list)\n",
    "df = pd.DataFrame(list(dictionary.items()), columns=[\"0\", \"word\"])\n",
    "df = df.drop([\"0\"], axis=1)\n",
    "# BAG OF WORDS\n",
    "corpus = [dictionary.doc2bow(paragraph) for paragraph in processed_list]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases<7454 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_model = Phrases(processed_list)\n",
    "trigram_model = Phrases(bigram_model[processed_list], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[processed_list]])\n",
    "print(bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=100)\\nmodel.save('ldamodel.gensim')\\ntopics = model.print_topics(num_words=7)\\nfor topic in topics:\\n    print(topic)\\n\\n# Compute Perplexity\\nprint('\\nPerplexity: ', model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=100)\n",
    "model.save('ldamodel.gensim')\n",
    "topics = model.print_topics(num_words=7)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.033*\"market\" + 0.031*\"project\" + 0.027*\"measur\" + 0.023*\"state\" + 0.022*\"research\"\n",
      "1: 0.062*\"research\" + 0.062*\"activ\" + 0.046*\"develop\" + 0.030*\"measur\" + 0.028*\"implement\"\n",
      "2: 0.042*\"implement\" + 0.037*\"oil_shale\" + 0.037*\"act\" + 0.033*\"govern\" + 0.031*\"use\"\n",
      "3: 0.049*\"fund\" + 0.032*\"servic\" + 0.030*\"project\" + 0.023*\"march\" + 0.020*\"support\"\n",
      "4: 0.057*\"construct\" + 0.043*\"prepar\" + 0.041*\"plan\" + 0.040*\"project\" + 0.032*\"implement\"\n",
      "5: 0.173*\"servic\" + 0.037*\"support\" + 0.029*\"develop\" + 0.023*\"provis\" + 0.020*\"transport\"\n",
      "6: 0.047*\"measur\" + 0.045*\"develop\" + 0.034*\"activ\" + 0.025*\"servic\" + 0.025*\"system\"\n",
      "7: 0.043*\"act\" + 0.042*\"busi\" + 0.036*\"student\" + 0.035*\"propos\" + 0.033*\"develop\"\n",
      "8: 0.032*\"economi\" + 0.028*\"energi\" + 0.027*\"measur\" + 0.026*\"data\" + 0.025*\"eu\"\n",
      "9: 0.063*\"project\" + 0.037*\"system\" + 0.032*\"develop\" + 0.031*\"research\" + 0.023*\"activ\"\n",
      "10: 0.063*\"develop\" + 0.043*\"measur\" + 0.037*\"plan\" + 0.032*\"implement\" + 0.026*\"resourc\"\n",
      "11: 0.037*\"support\" + 0.035*\"project\" + 0.032*\"road\" + 0.029*\"educ\" + 0.027*\"system\"\n",
      "12: 0.088*\"activ\" + 0.037*\"transport\" + 0.028*\"train\" + 0.025*\"system\" + 0.022*\"plan\"\n",
      "13: 0.038*\"develop\" + 0.032*\"servic\" + 0.018*\"system\" + 0.018*\"educ\" + 0.017*\"procur\"\n",
      "14: 0.039*\"activ\" + 0.031*\"programm\" + 0.030*\"implement\" + 0.027*\"measur\" + 0.024*\"develop\"\n",
      "15: 0.096*\"activ\" + 0.088*\"programm\" + 0.070*\"school\" + 0.045*\"develop\" + 0.020*\"year\"\n",
      "16: 0.048*\"school\" + 0.045*\"servic\" + 0.039*\"measur\" + 0.036*\"educ\" + 0.030*\"studi\"\n",
      "17: 0.069*\"measur\" + 0.042*\"research\" + 0.037*\"school\" + 0.030*\"develop\" + 0.024*\"infrastructur\"\n",
      "18: 0.070*\"develop\" + 0.042*\"peopl\" + 0.035*\"research\" + 0.027*\"system\" + 0.025*\"support\"\n",
      "19: 0.045*\"inform\" + 0.039*\"develop\" + 0.038*\"specialist\" + 0.036*\"order\" + 0.035*\"invest\"\n",
      "\n",
      "Perplexity:  -7.190289774533179\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "num_topics = 20\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=3, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "\n",
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=5):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    \n",
    "    \n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # one time execution\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.update()\n",
    "paragraphs_list = []\n",
    "processed_list =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = parser.from_file('nrp_estonia_2020_action_plan_2019-2020_30.05.2019.pdf')\n",
    "pdf_text = pdf['content']\n",
    "pdf_text = pdf_text.lower()\n",
    "pdf_text = re.sub(\"x \", \"\", pdf_text)\n",
    "pdf_text = ' '.join(pdf_text.split())\n",
    "pdf_text = pdf_text.split(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pdf_text[0:2]\n",
    "sentences = pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to establish a support scheme for involving the development workers in companies that make a significant contribution to employment but currently have low added value ministry of economic affairs and communications the development voucher measure is still open\n",
      "to increase the motivation of universities and businesses to diversity financing sources, by making use of the research and development needs of companies located outside of estonia, but also the eu’s various r&d&i financing programmes (e.g., horizon 2020, era-net, jti, kic etc.)\n",
      "activities: a) information about coming to work in estonia, development and maintenance of a common e-environment of information, materials, and public services; b) increasing readiness of entrepreneurs to involve foreign specialists; c) it labour force campaigns in target countries, including the industrial sector; d) development of settlement services for foreign specialists and their families\n",
      "creating a system for assessing work capacity, the provision of services to the target group, the necessary information exchange it solutions for implementing the activities and condition for providing grants “the provision of labour market services to the work capacity reform target group” and “increasing the work capacity of the work capacity reform target group and promoting their working”) ministry of social affairs 1 the development and outreach activities required to ensure the sustainability of the new work ability support system are in progress and analyses are carried out to check the effectiveness of the system and detect areas which require further development\n",
      "ministry of economic affairs and communications the measure “the conditions and procedure for use of support intended to aid the development of biomethane market” is being implemented and as the result of the implementation of the measure, biomethane will be made available for end-users for the same price as natural gas\n",
      "development and testing of the youth guarantee support system to identify young people who are inactive without a reason, including neet youths, with the help of state registers and offering them help to continue their studies or to move into the labour market ministry of social affairs in 2018, the youth guarantee support system was created, which compiles a list for local governments of young people in their area who potentially need assistance (who do not work or study) based on register data, to offer them support in returning to the education system or employment\n",
      "development of an electronic data exchange related to social insurance between the eu member states, so that people would be assured of rapid solutions for cross-border cases related to social insurance ministry of social affairs estonia officially joined the eessi platform in 2017; however, it was not possible to interface with the platform in 2017\n",
      "proactively analysing the legal framework and promotion of the state information system, in order to support the initial testing and introduction of smart technologies (e.g., the internet of things, linked data) ministry of economic affairs and communications this work is continuous\n",
      "to support the last mile construction, the ministry of economic affairs and communications prepared a support measure and the consumer protection and technical regulatory authority organised a public procurement, which was won by elektrilevi oü, who will construct in 2019–2023 the possibilities to connect via optical lines for the addresses situated all over estonia in the areas of market failure, costing prescribed support in the amount of 20 million euros\n",
      "to develop a measure aimed at enterprises to encourage the use of r&d infrastructure, with the aim to increase the interest and possibilities of entrepreneurs in using the r&d infrastructure in the product development process ministry of economic affairs and communications, ministry of education and research 2 the estonian research council launched the so-called core infrastructures measure within the institutional research aid scheme\n"
     ]
    }
   ],
   "source": [
    "# Extract top 10 sentences as the summary\n",
    "for i in range(10):\n",
    "  print(ranked_sentences[i][1])\n",
    "#https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
